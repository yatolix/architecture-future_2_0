PS D:\architect\sprint11\architecture-future_2_0\Task4> terraform plan
data.yandex_compute_image.ubuntu: Reading...
data.yandex_compute_image.ubuntu: Read complete after 0s [id=fd8t9g30r3pc23et5krl]

Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # yandex_compute_instance.ai will be created
  + resource "yandex_compute_instance" "ai" {
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + gpu_cluster_id            = (known after apply)
      + hardware_generation       = (known after apply)
      + hostname                  = (known after apply)
      + id                        = (known after apply)
      + maintenance_grace_period  = (known after apply)
      + maintenance_policy        = (known after apply)
      + metadata                  = {
          + "user-data" = <<-EOT
                #cloud-config
                users:
                  - name: ubuntu
                    ssh-authorized-keys:
                      - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBGg0fe7yk/99jfk89cDvPnoFC555yDlUBkycJEmolUY tolis@Tolishok
                    sudo: ['ALL=(ALL) NOPASSWD:ALL']
                    shell: /bin/bash
            EOT
        }
      + name                      = "ai-services"
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v2"
      + status                    = (known after apply)
      + zone                      = "ru-central1-a"

      + boot_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + block_size  = (known after apply)
              + description = (known after apply)
              + image_id    = "fd8t9g30r3pc23et5krl"
              + name        = (known after apply)
              + size        = 20
              + snapshot_id = (known after apply)
              + type        = "network-hdd"
            }
        }

      + metadata_options (known after apply)

      + network_interface {
          + index          = (known after apply)
          + ip_address     = (known after apply)
          + ipv4           = true
          + ipv6           = (known after apply)
          + ipv6_address   = (known after apply)
          + mac_address    = (known after apply)
          + nat            = true
          + nat_ip_address = (known after apply)
          + nat_ip_version = (known after apply)
          + subnet_id      = (known after apply)
        }

      + placement_policy (known after apply)

      + resources {
          + core_fraction = 20
          + cores         = 2
          + memory        = 2
        }

      + scheduling_policy (known after apply)
    }

  # yandex_compute_instance.airflow will be created
  + resource "yandex_compute_instance" "airflow" {
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + gpu_cluster_id            = (known after apply)
      + hardware_generation       = (known after apply)
      + hostname                  = (known after apply)
      + id                        = (known after apply)
      + maintenance_grace_period  = (known after apply)
      + maintenance_policy        = (known after apply)
      + metadata                  = {
          + "user-data" = <<-EOT
                #cloud-config
                users:
                  - name: ubuntu
                    ssh-authorized-keys:
                      - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBGg0fe7yk/99jfk89cDvPnoFC555yDlUBkycJEmolUY tolis@Tolishok
                    sudo: ['ALL=(ALL) NOPASSWD:ALL']
                    shell: /bin/bash
            EOT
        }
      + name                      = "airflow-vm"
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v2"
      + status                    = (known after apply)
      + zone                      = "ru-central1-a"

      + boot_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + block_size  = (known after apply)
              + description = (known after apply)
              + image_id    = "fd8t9g30r3pc23et5krl"
              + name        = (known after apply)
              + size        = 20
              + snapshot_id = (known after apply)
              + type        = "network-hdd"
            }
        }

      + metadata_options (known after apply)

      + network_interface {
          + index          = (known after apply)
          + ip_address     = (known after apply)
          + ipv4           = true
          + ipv6           = (known after apply)
          + ipv6_address   = (known after apply)
          + mac_address    = (known after apply)
          + nat            = true
          + nat_ip_address = (known after apply)
          + nat_ip_version = (known after apply)
          + subnet_id      = (known after apply)
        }

      + placement_policy (known after apply)

      + resources {
          + core_fraction = 20
          + cores         = 2
          + memory        = 2
        }

      + scheduling_policy (known after apply)
    }

  # yandex_compute_instance.fin will be created
  + resource "yandex_compute_instance" "fin" {
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + gpu_cluster_id            = (known after apply)
      + hardware_generation       = (known after apply)
      + hostname                  = (known after apply)
      + id                        = (known after apply)
      + maintenance_grace_period  = (known after apply)
      + maintenance_policy        = (known after apply)
      + metadata                  = {
          + "user-data" = <<-EOT
                #cloud-config
                users:
                  - name: ubuntu
                    ssh-authorized-keys:
                      - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBGg0fe7yk/99jfk89cDvPnoFC555yDlUBkycJEmolUY tolis@Tolishok
                    sudo: ['ALL=(ALL) NOPASSWD:ALL']
                    shell: /bin/bash
            EOT
        }
      + name                      = "fin-services"
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v2"
      + status                    = (known after apply)
      + zone                      = "ru-central1-a"

      + boot_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + block_size  = (known after apply)
              + description = (known after apply)
              + image_id    = "fd8t9g30r3pc23et5krl"
              + name        = (known after apply)
              + size        = 20
              + snapshot_id = (known after apply)
              + type        = "network-hdd"
            }
        }

      + metadata_options (known after apply)

      + network_interface {
          + index          = (known after apply)
          + ip_address     = (known after apply)
          + ipv4           = true
          + ipv6           = (known after apply)
          + ipv6_address   = (known after apply)
          + mac_address    = (known after apply)
          + nat            = true
          + nat_ip_address = (known after apply)
          + nat_ip_version = (known after apply)
          + subnet_id      = (known after apply)
        }

      + placement_policy (known after apply)

      + resources {
          + core_fraction = 20
          + cores         = 2
          + memory        = 2
        }

      + scheduling_policy (known after apply)
    }

  # yandex_compute_instance.mdm will be created
  + resource "yandex_compute_instance" "mdm" {
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + gpu_cluster_id            = (known after apply)
      + hardware_generation       = (known after apply)
      + hostname                  = (known after apply)
      + id                        = (known after apply)
      + maintenance_grace_period  = (known after apply)
      + maintenance_policy        = (known after apply)
      + metadata                  = {
          + "user-data" = <<-EOT
                #cloud-config
                users:
                  - name: ubuntu
                    ssh-authorized-keys:
                      - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBGg0fe7yk/99jfk89cDvPnoFC555yDlUBkycJEmolUY tolis@Tolishok
                    sudo: ['ALL=(ALL) NOPASSWD:ALL']
                    shell: /bin/bash
            EOT
        }
      + name                      = "mdm-services"
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v2"
      + status                    = (known after apply)
      + zone                      = "ru-central1-a"

      + boot_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + block_size  = (known after apply)
              + description = (known after apply)
              + image_id    = "fd8t9g30r3pc23et5krl"
              + name        = (known after apply)
              + size        = 20
              + snapshot_id = (known after apply)
              + type        = "network-hdd"
            }
        }

      + metadata_options (known after apply)

      + network_interface {
          + index          = (known after apply)
          + ip_address     = (known after apply)
          + ipv4           = true
          + ipv6           = (known after apply)
          + ipv6_address   = (known after apply)
          + mac_address    = (known after apply)
          + nat            = true
          + nat_ip_address = (known after apply)
          + nat_ip_version = (known after apply)
          + subnet_id      = (known after apply)
        }

      + placement_policy (known after apply)

      + resources {
          + core_fraction = 20
          + cores         = 2
          + memory        = 2
        }

      + scheduling_policy (known after apply)
    }

  # yandex_compute_instance.med will be created
  + resource "yandex_compute_instance" "med" {
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + gpu_cluster_id            = (known after apply)
      + hardware_generation       = (known after apply)
      + hostname                  = (known after apply)
      + id                        = (known after apply)
      + maintenance_grace_period  = (known after apply)
      + maintenance_policy        = (known after apply)
      + metadata                  = {
          + "user-data" = <<-EOT
                #cloud-config
                users:
                  - name: ubuntu
                    ssh-authorized-keys:
                      - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBGg0fe7yk/99jfk89cDvPnoFC555yDlUBkycJEmolUY tolis@Tolishok
                    sudo: ['ALL=(ALL) NOPASSWD:ALL']
                    shell: /bin/bash
            EOT
        }
      + name                      = "med-services"
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v2"
      + status                    = (known after apply)
      + zone                      = "ru-central1-a"

      + boot_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + block_size  = (known after apply)
              + description = (known after apply)
              + image_id    = "fd8t9g30r3pc23et5krl"
              + name        = (known after apply)
              + size        = 20
              + snapshot_id = (known after apply)
              + type        = "network-hdd"
            }
        }

      + metadata_options (known after apply)

      + network_interface {
          + index          = (known after apply)
          + ip_address     = (known after apply)
          + ipv4           = true
          + ipv6           = (known after apply)
          + ipv6_address   = (known after apply)
          + mac_address    = (known after apply)
          + nat            = true
          + nat_ip_address = (known after apply)
          + nat_ip_version = (known after apply)
          + subnet_id      = (known after apply)
        }

      + placement_policy (known after apply)

      + resources {
          + core_fraction = 20
          + cores         = 2
          + memory        = 2
        }

      + scheduling_policy (known after apply)
    }

  # yandex_compute_instance.portal will be created
  + resource "yandex_compute_instance" "portal" {
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + gpu_cluster_id            = (known after apply)
      + hardware_generation       = (known after apply)
      + hostname                  = (known after apply)
      + id                        = (known after apply)
      + maintenance_grace_period  = (known after apply)
      + maintenance_policy        = (known after apply)
      + metadata                  = {
          + "user-data" = <<-EOT
                #cloud-config
                users:
                  - name: ubuntu
                    ssh-authorized-keys:
                      - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBGg0fe7yk/99jfk89cDvPnoFC555yDlUBkycJEmolUY tolis@Tolishok
                    sudo: ['ALL=(ALL) NOPASSWD:ALL']
                    shell: /bin/bash
            EOT
        }
      + name                      = "portal-services"
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v2"
      + status                    = (known after apply)
      + zone                      = "ru-central1-a"

      + boot_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + block_size  = (known after apply)
              + description = (known after apply)
              + image_id    = "fd8t9g30r3pc23et5krl"
              + name        = (known after apply)
              + size        = 30
              + snapshot_id = (known after apply)
              + type        = "network-hdd"
            }
        }

      + metadata_options (known after apply)

      + network_interface {
          + index          = (known after apply)
          + ip_address     = (known after apply)
          + ipv4           = true
          + ipv6           = (known after apply)
          + ipv6_address   = (known after apply)
          + mac_address    = (known after apply)
          + nat            = true
          + nat_ip_address = (known after apply)
          + nat_ip_version = (known after apply)
          + subnet_id      = (known after apply)
        }

      + placement_policy (known after apply)

      + resources {
          + core_fraction = 20
          + cores         = 2
          + memory        = 4
        }

      + scheduling_policy (known after apply)
    }

  # yandex_iam_service_account.storage_sa will be created
  + resource "yandex_iam_service_account" "storage_sa" {
      + created_at         = (known after apply)
      + description        = (known after apply)
      + folder_id          = (known after apply)
      + id                 = (known after apply)
      + labels             = (known after apply)
      + name               = "storage-sa"
      + service_account_id = (known after apply)
    }

  # yandex_iam_service_account_static_access_key.storage_key will be created
  + resource "yandex_iam_service_account_static_access_key" "storage_key" {
      + access_key                   = (known after apply)
      + created_at                   = (known after apply)
      + encrypted_secret_key         = (known after apply)
      + id                           = (known after apply)
      + key_fingerprint              = (known after apply)
      + output_to_lockbox_version_id = (known after apply)
      + secret_key                   = (sensitive value)
      + service_account_id           = (known after apply)
    }

  # yandex_mdb_kafka_cluster.kafka will be created
  + resource "yandex_mdb_kafka_cluster" "kafka" {
      + created_at             = (known after apply)
      + deletion_protection    = (known after apply)
      + disk_encryption_key_id = (known after apply)
      + environment            = "PRODUCTION"
      + folder_id              = (known after apply)
      + health                 = (known after apply)
      + host                   = (known after apply)
      + host_group_ids         = (known after apply)
      + id                     = (known after apply)
      + labels                 = (known after apply)
      + name                   = "future2-kafka"
      + network_id             = (known after apply)
      + security_group_ids     = (known after apply)
      + status                 = (known after apply)
      + subnet_ids             = (known after apply)

      + config {
          + assign_public_ip = false
          + brokers_count    = 1
          + patch_version    = (known after apply)
          + schema_registry  = false
          + unmanaged_topics = false
          + version          = "3.6"
          + zones            = [
              + "ru-central1-a",
            ]

          + access (known after apply)

          + disk_size_autoscaling (known after apply)

          + kafka {
              + resources {
                  + disk_size          = 10
                  + disk_type_id       = "network-hdd"
                  + resource_preset_id = "b2.medium"
                }
            }

          + kafka_ui (known after apply)

          + kraft (known after apply)

          + rest_api (known after apply)

          + zookeeper (known after apply)
        }

      + maintenance_window (known after apply)
    }

  # yandex_mdb_postgresql_cluster.fin will be created
  + resource "yandex_mdb_postgresql_cluster" "fin" {
      + created_at             = (known after apply)
      + deletion_protection    = (known after apply)
      + disk_encryption_key_id = (known after apply)
      + environment            = "PRODUCTION"
      + folder_id              = (known after apply)
      + health                 = (known after apply)
      + host_group_ids         = (known after apply)
      + host_master_name       = (known after apply)
      + id                     = (known after apply)
      + labels                 = (known after apply)
      + name                   = "fin-postgres"
      + network_id             = (known after apply)
      + security_group_ids     = (known after apply)
      + status                 = (known after apply)

      + config {
          + autofailover              = (known after apply)
          + backup_retain_period_days = (known after apply)
          + postgresql_config         = (known after apply)
          + version                   = "15"

          + access (known after apply)

          + backup_window_start (known after apply)

          + disk_size_autoscaling (known after apply)

          + performance_diagnostics (known after apply)

          + resources {
              + disk_size          = 10
              + disk_type_id       = "network-hdd"
              + resource_preset_id = "b2.medium"
            }
        }

      + host {
          + assign_public_ip   = false
          + fqdn               = (known after apply)
          + replication_source = (known after apply)
          + role               = (known after apply)
          + subnet_id          = (known after apply)
          + zone               = "ru-central1-a"
        }

      + maintenance_window (known after apply)
    }

  # yandex_mdb_postgresql_cluster.mdm will be created
  + resource "yandex_mdb_postgresql_cluster" "mdm" {
      + created_at             = (known after apply)
      + deletion_protection    = (known after apply)
      + disk_encryption_key_id = (known after apply)
      + environment            = "PRODUCTION"
      + folder_id              = (known after apply)
      + health                 = (known after apply)
      + host_group_ids         = (known after apply)
      + host_master_name       = (known after apply)
      + id                     = (known after apply)
      + labels                 = (known after apply)
      + name                   = "mdm-postgres"
      + network_id             = (known after apply)
      + security_group_ids     = (known after apply)
      + status                 = (known after apply)

      + config {
          + autofailover              = (known after apply)
          + backup_retain_period_days = (known after apply)
          + postgresql_config         = (known after apply)
          + version                   = "15"

          + access (known after apply)

          + backup_window_start (known after apply)

          + disk_size_autoscaling (known after apply)

          + performance_diagnostics (known after apply)

          + resources {
              + disk_size          = 10
              + disk_type_id       = "network-hdd"
              + resource_preset_id = "b2.medium"
            }
        }

      + host {
          + assign_public_ip   = false
          + fqdn               = (known after apply)
          + replication_source = (known after apply)
          + role               = (known after apply)
          + subnet_id          = (known after apply)
          + zone               = "ru-central1-a"
        }

      + maintenance_window (known after apply)
    }

  # yandex_mdb_postgresql_cluster.med will be created
  + resource "yandex_mdb_postgresql_cluster" "med" {
      + created_at             = (known after apply)
      + deletion_protection    = (known after apply)
      + disk_encryption_key_id = (known after apply)
      + environment            = "PRODUCTION"
      + folder_id              = (known after apply)
      + health                 = (known after apply)
      + host_group_ids         = (known after apply)
      + host_master_name       = (known after apply)
      + id                     = (known after apply)
      + labels                 = (known after apply)
      + name                   = "med-postgres"
      + network_id             = (known after apply)
      + security_group_ids     = (known after apply)
      + status                 = (known after apply)

      + config {
          + autofailover              = (known after apply)
          + backup_retain_period_days = (known after apply)
          + postgresql_config         = (known after apply)
          + version                   = "15"

          + access (known after apply)

          + backup_window_start (known after apply)

          + disk_size_autoscaling (known after apply)

          + performance_diagnostics (known after apply)

          + resources {
              + disk_size          = 10
              + disk_type_id       = "network-hdd"
              + resource_preset_id = "b2.medium"
            }
        }

      + host {
          + assign_public_ip   = false
          + fqdn               = (known after apply)
          + replication_source = (known after apply)
          + role               = (known after apply)
          + subnet_id          = (known after apply)
          + zone               = "ru-central1-a"
        }

      + maintenance_window (known after apply)
    }

  # yandex_mdb_postgresql_database.fin will be created
  + resource "yandex_mdb_postgresql_database" "fin" {
      + cluster_id          = (known after apply)
      + deletion_protection = "unspecified"
      + id                  = (known after apply)
      + lc_collate          = "C"
      + lc_type             = "C"
      + name                = "findb"
      + owner               = "finuser"
    }

  # yandex_mdb_postgresql_database.mdm will be created
  + resource "yandex_mdb_postgresql_database" "mdm" {
      + cluster_id          = (known after apply)
      + deletion_protection = "unspecified"
      + id                  = (known after apply)
      + lc_collate          = "C"
      + lc_type             = "C"
      + name                = "mdmdb"
      + owner               = "mdmuser"
    }

  # yandex_mdb_postgresql_database.med will be created
  + resource "yandex_mdb_postgresql_database" "med" {
      + cluster_id          = (known after apply)
      + deletion_protection = "unspecified"
      + id                  = (known after apply)
      + lc_collate          = "C"
      + lc_type             = "C"
      + name                = "meddb"
      + owner               = "meduser"
    }

  # yandex_mdb_postgresql_user.fin will be created
  + resource "yandex_mdb_postgresql_user" "fin" {
      + auth_method              = "AUTH_METHOD_PASSWORD"
      + cluster_id               = (known after apply)
      + conn_limit               = (known after apply)
      + connection_manager       = (known after apply)
      + deletion_protection      = "unspecified"
      + generate_password        = false
      + id                       = (known after apply)
      + login                    = true
      + name                     = "finuser"
      + password                 = (sensitive value)
      + user_password_encryption = (known after apply)
    }

  # yandex_mdb_postgresql_user.mdm will be created
  + resource "yandex_mdb_postgresql_user" "mdm" {
      + auth_method              = "AUTH_METHOD_PASSWORD"
      + cluster_id               = (known after apply)
      + conn_limit               = (known after apply)
      + connection_manager       = (known after apply)
      + deletion_protection      = "unspecified"
      + generate_password        = false
      + id                       = (known after apply)
      + login                    = true
      + name                     = "mdmuser"
      + password                 = (sensitive value)
      + user_password_encryption = (known after apply)
    }

  # yandex_mdb_postgresql_user.med will be created
  + resource "yandex_mdb_postgresql_user" "med" {
      + auth_method              = "AUTH_METHOD_PASSWORD"
      + cluster_id               = (known after apply)
      + conn_limit               = (known after apply)
      + connection_manager       = (known after apply)
      + deletion_protection      = "unspecified"
      + generate_password        = false
      + id                       = (known after apply)
      + login                    = true
      + name                     = "meduser"
      + password                 = (sensitive value)
      + user_password_encryption = (known after apply)
    }

  # yandex_resourcemanager_folder_iam_member.storage_admin will be created
  + resource "yandex_resourcemanager_folder_iam_member" "storage_admin" {
      + folder_id = "b1gkdugc7l09tci2crft"
      + id        = (known after apply)
      + member    = (known after apply)
      + role      = "storage.admin"
    }

  # yandex_storage_bucket.lakehouse will be created
  + resource "yandex_storage_bucket" "lakehouse" {
      + access_key            = (known after apply)
      + acl                   = (known after apply)
      + bucket                = "future2-lakehouse-b1gkdugc7l09tci2crft"
      + bucket_domain_name    = (known after apply)
      + default_storage_class = (known after apply)
      + folder_id             = (known after apply)
      + force_destroy         = true
      + id                    = (known after apply)
      + policy                = (known after apply)
      + secret_key            = (sensitive value)
      + website_domain        = (known after apply)
      + website_endpoint      = (known after apply)

      + anonymous_access_flags (known after apply)

      + grant (known after apply)

      + versioning (known after apply)
    }

  # yandex_vpc_network.network will be created
  + resource "yandex_vpc_network" "network" {
      + created_at                = (known after apply)
      + default_security_group_id = (known after apply)
      + folder_id                 = (known after apply)
      + id                        = (known after apply)
      + labels                    = (known after apply)
      + name                      = "future2-network"
      + subnet_ids                = (known after apply)
    }

  # yandex_vpc_subnet.subnet will be created
  + resource "yandex_vpc_subnet" "subnet" {
      + created_at     = (known after apply)
      + folder_id      = (known after apply)
      + id             = (known after apply)
      + labels         = (known after apply)
      + name           = "future2-subnet"
      + network_id     = (known after apply)
      + v4_cidr_blocks = [
          + "10.10.0.0/24",
        ]
      + v6_cidr_blocks = (known after apply)
      + zone           = "ru-central1-a"
    }

Plan: 22 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + ai_vm_ip      = (known after apply)
  + airflow_vm_ip = (known after apply)
  + bucket_name   = "future2-lakehouse-b1gkdugc7l09tci2crft"
  + fin_db_host   = (known after apply)
  + fin_vm_ip     = (known after apply)
  + kafka_brokers = (known after apply)
  + mdm_db_host   = (known after apply)
  + mdm_vm_ip     = (known after apply)
  + med_db_host   = (known after apply)
  + med_vm_ip     = (known after apply)
  + portal_vm_ip  = (known after apply)